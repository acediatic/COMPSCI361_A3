{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd0e4329b641bbdbd8e1182fbadc5ebb2e523e312e1f9373a057c466e1263c44057",
   "display_name": "Python 3.9.5 64-bit (windows store)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e4329b641bbdbd8e1182fbadc5ebb2e523e312e1f9373a057c466e1263c44057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "p()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"naivebayes-21\\\\trg.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = []\n",
    "class_freq = defaultdict(lambda: 0)\n",
    "corpus = []\n",
    "\n",
    "with open(PATH_TO_DATA) as csv_file:\n",
    "    for line in csv_file:\n",
    "        line = line.replace('\\\"', '')\n",
    "\n",
    "        line_lst = line.split(',')\n",
    "        line_lst[-1] = line_lst[-1].replace('\\n', '')\n",
    "\n",
    "        class_freq[line_lst[1]] += 1 \n",
    "\n",
    "        corpus.append(line_lst[-1])\n",
    "\n",
    "        list_data.append(line_lst)\n",
    "\n",
    "\n",
    "full_csv_data = np.array(list_data)\n",
    "class_freq.pop('class')\n",
    "\n",
    "\n",
    "labels = class_freq.keys()\n",
    "headers = full_csv_data[0]\n",
    "data = np.asarray(full_csv_data[1:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_items([('B', 1602), ('A', 128), ('E', 2144), ('V', 126)])\ndict_keys(['B', 'A', 'E', 'V'])\n"
     ]
    }
   ],
   "source": [
    "print(class_freq.items())\n",
    "print(labels)\n",
    "\n",
    "NUM_CLASSES = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "ABSTRT_I = 1\n",
    "LBL_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances_for_class = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_stratified_kfold_splits(data, k=10):\n",
    "    instances_of_class = dict()\n",
    "\n",
    "    # Create a dictionary with key being label, and the value being an array of instances of that label. \n",
    "    for i, label in enumerate(labels):\n",
    "        i_arr = data[:, 0] == label\n",
    "        instances_of_class[label] = data[i_arr]\n",
    "        num_instances_for_class[i] = len(instances_of_class[label])\n",
    "    \n",
    "    stratified_splits = dict()\n",
    "    \n",
    "    # Upsample to divisible by k\n",
    "    for class_label, class_instances in instances_of_class.items():\n",
    "        n = len(class_instances)\n",
    "        upsample_amt = k - (n % k)\n",
    "\n",
    "        random_indices = np.random.choice(class_instances[:, ABSTRT_I], size=upsample_amt, replace=False)\n",
    "        random_indices.resize((random_indices.shape[0], 2), refcheck=False)\n",
    "\n",
    "        random_indices[:, -1] = class_label\n",
    "\n",
    "        random_indices[:,[0, 1]] = random_indices[:,[1, 0]]\n",
    "\n",
    "        upsampled_class_instances = np.concatenate((class_instances, random_indices), 0)\n",
    "        assert len(upsampled_class_instances) % k == 0, \"num examples should be divisible by k\"\n",
    "\n",
    "        stratified_splits[class_label] = np.split(upsampled_class_instances, k)\n",
    "\n",
    "    for i in range(k):\n",
    "        kth_test_lst = [stratified_splits[label][i] for label in labels]\n",
    "        kth_train_lst = [stratified_splits[label][j] for label in labels for j in range(k) if j != i]\n",
    "        \n",
    "        kth_train_data = np.concatenate(kth_train_lst)\n",
    "        kth_test_data = np.concatenate(kth_test_lst)\n",
    "        np.random.shuffle(kth_train_data)\n",
    "        np.random.shuffle(kth_test_data)\n",
    "\n",
    "        yield kth_train_data, kth_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_data = get_stratified_kfold_splits(data)\n",
    "label_ints = {lbl:i for i, lbl in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_all_words(X):\n",
    "    all_words = set(word for i in range(len(data)) for word in X[i].split() if word) \n",
    "\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_i(all_words):\n",
    "    return {word:i for i, word in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(data):\n",
    "    X = data[:, ABSTRT_I]\n",
    "    return get_all_words_i(get_set_all_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(data, all_words_i):    \n",
    "    # ALPHA = 1\n",
    "    freq_mat = np.ones((len(data), len(all_words_i)+1))\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i][ABSTRT_I].split():\n",
    "            try:\n",
    "                freq_mat[i,all_words_i[word]] += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        freq_mat[i, -1] = label_ints[data[i, LBL_I]]\n",
    "\n",
    "    return freq_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(arr, i):\n",
    "    ''' Calcualtes the correlation between one column (class) and the rest for the input matrix. Credit to FBruzzesi (https://stackoverflow.com/users/12411536/fbruzzesi)'''\n",
    "    mean_t = np.mean(arr, axis=0)\n",
    "    std_t = np.std(arr, axis=0)\n",
    "\n",
    "    mean_i = mean_t[i]\n",
    "    std_i = std_t[i]\n",
    "\n",
    "    mean_xy = np.mean(arr*arr[:,i][:,None], axis=0)\n",
    "\n",
    "    corr = (mean_xy - mean_i * mean_t)/(std_i * std_t)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_best(freq_mat, k=10):\n",
    "    R = corr(freq_mat, -1)\n",
    "    class_correlations = abs(R)\n",
    "\n",
    "    k_best_i = np.argpartition(class_correlations, -(k+1))[-(k+1):]\n",
    "    \n",
    "    # removes self column\n",
    "    k_best_i = k_best_i[:-1]\n",
    "\n",
    "    return k_best_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_clf_form(freq_mat, k_best_i):\n",
    "    X = freq_mat[:, k_best_i]\n",
    "    y = freq_mat[:, -1]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_mat(X, y):\n",
    "    sum_mat = np.zeros(NUM_CLASSES, X.shape[1])\n",
    "\n",
    "    for i in range(NUM_CLASSES):\n",
    "        temp = X[y == i]\n",
    "        sum_mat[i,:] = np.sum(temp, axis=0)\n",
    "\n",
    "    return sum_mat, np.sum(sum_mat, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq_k_best(train, test):\n",
    "    all_words_i = get_all_words(train)\n",
    "\n",
    "    freq_mat_train = get_word_counts(train, all_words_i)\n",
    "    k_best_i = select_k_best(freq_mat_train, NUM_FEATURES)\n",
    "\n",
    "    freq_mat_test = get_word_counts(test, all_words_i)\n",
    "\n",
    "    X_train, y_train = convert_to_clf_form(freq_mat_train, k_best_i)\n",
    "    X_test, y_test = convert_to_clf_form(freq_mat_test, k_best_i)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF\n",
    "def abstract_to_dict(abstract):\n",
    "    '''word:count for words in a particular abstract'''\n",
    "    abstractDict = defaultdict(lambda: 0)\n",
    "    for word in abstract.split(' '):\n",
    "        if word:\n",
    "            abstractDict[word] += 1 \n",
    "    return abstractDict \n",
    "\n",
    "def termFrequency(abstractDict : dict):\n",
    "    \"\"\"(# of repetitions of word in a document) / (# of words in a document)\"\"\"\n",
    "    termFrequencies = {}\n",
    "    numWords = len(abstractDict)\n",
    "    \n",
    "    for word, count in abstractDict.items():\n",
    "        termFrequencies[word] = count/numWords\n",
    "    return termFrequencies\n",
    "\n",
    "def get_inverse_document_frequency(abstract_dict_list : list):\n",
    "    \"\"\" used to calculate the weight of rare words across all documents in the corpus\n",
    "        idf(w) = log(num_docs/freq_word_all_docs)\"\"\"\n",
    "    idf = defaultdict(lambda: 0)\n",
    "    numAbstracts = len(abstract_dict_list)\n",
    "\n",
    "    # calculte number of docs containing word\n",
    "    for abstractDict in abstract_dict_list:\n",
    "        for word, count in abstractDict.items():\n",
    "            if count > 0:\n",
    "                idf[word] += 1 \n",
    "\n",
    "    from math import log10\n",
    "    for word, num_docs_containing_word in idf.items():\n",
    "        idf[word] = log10(numAbstracts/float(num_docs_containing_word))\n",
    "\n",
    "    return idf\n",
    "\n",
    "def get_row_tfidf(tf, idf, word_indexes):\n",
    "    ''' num occurrences of word i in doc j * log(total docs / number of documents containing i) '''\n",
    "    row = np.zeros((1, len(word_indexes)))\n",
    "    for word, numOccurences in tf.items():\n",
    "        word_index = word_indexes[word]\n",
    "        row[0, word_index] = numOccurences*idf[word] \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(X):\n",
    "    abstract_dict_list = []\n",
    "    term_frequency_lst = []\n",
    "    all_words = get_set_all_words(X)\n",
    "\n",
    "    new_X = np.zeros((len(X), len(all_words)))\n",
    "    word_indexes = {word: i for i, word in enumerate(all_words)}\n",
    "\n",
    "    for abstract in X:\n",
    "        abstract_dict = abstract_to_dict(abstract)\n",
    "        abstract_dict_list.append(abstract_dict)\n",
    "        term_frequency_lst.append(termFrequency(abstract_dict))\n",
    "\n",
    "    print(\"converted abstracts, calculating idf\")    \n",
    "    idf = get_inverse_document_frequency(abstract_dict_list)\n",
    "\n",
    "    print(\"calculating tfidf\")\n",
    "    for i,tf in enumerate(term_frequency_lst):\n",
    "        if (i % 500 == 0):\n",
    "            print(\"{}% complete\".format(i/len(abstract_dict_list) * 100))\n",
    "        new_X[i, :] = get_row_tfidf(tf, idf, word_indexes)\n",
    "    print(\"100% complete\")\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "converted abstracts, calculating idf\n",
      "calculating tfidf\n",
      "0.0% complete\n",
      "12.5% complete\n",
      "25.0% complete\n",
      "37.5% complete\n",
      "50.0% complete\n",
      "62.5% complete\n",
      "75.0% complete\n",
      "87.5% complete\n",
      "100% complete\n"
     ]
    }
   ],
   "source": [
    "X = calculate_tfidf(data[:, ABSTRT_I])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(X[X > 0].any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 7048 24988  7534   426 17796 18526 27190 18407 29270 25025  1918 14526\n 21735 12907 10027 11883  5976 19889 29256 19536]\n"
     ]
    }
   ],
   "source": [
    "sum_idf = X.sum(axis=0)\n",
    "k = 20\n",
    "k_best_i = np.argpartition(sum_idf, -k)[-k:]\n",
    "print(k_best_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, k_best_i]"
   ]
  },
  {
   "source": [
    "\"https://towardsdatascience.com/comparing-a-variety-of-naive-bayes-classification-algorithms-fc5fa298379e\"  \n",
    "https://geoffruddock.com/naive-bayes-from-scratch-with-numpy/\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'matrix_like' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-afb7713adf49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\" DIY implementation of binary Naive Bayes classifier based on categorical data\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;34m\"\"\" \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-116-afb7713adf49>\u001b[0m in \u001b[0;36mNaiveBayes\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconditional_probas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmatrix_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;34m\"\"\" Predict class with highest probability \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'matrix_like' is not defined"
     ]
    }
   ],
   "source": [
    "class NaiveBayes(object):\n",
    "    \"\"\" DIY implementation of binary Naive Bayes classifier based on categorical data\"\"\"\n",
    "    def __init__(self, alpha=1.0):\n",
    "        \"\"\" \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.prior = None\n",
    "        self.word_freq = None\n",
    "        self.word_p = None\n",
    "        self.num_classes = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = len(X)\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            i_arr = data[:, 0] == label\n",
    "            instances_of_class[label] = data[i_arr]\n",
    "            self.num_classes += 1\n",
    "\n",
    "        self.prior = np.array([len(instances_of_class[label]) / n for label in instances_of_class])\n",
    "\n",
    "        self.word_freq = np.array([class_arr.sum(axis=0) for class_arr in instances_of_class]) + self.alpha\n",
    "\n",
    "        self.lk_word = self.word_freq / self.word_freq.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Predict probability of class membership \"\"\"\n",
    "\n",
    "        # loop over each observation to calculate conditional probabilities\n",
    "        class_numerators = np.zeros((len(X), self.num_classes))\n",
    "        for i, x in enumerate(X):\n",
    "            word_exists = x.astype(bool)\n",
    "            lk_words_present = self.lk_word[:, word_exists] ** x[word_exists]\n",
    "            lk_message = (lk_words_present).prod(axis=1)\n",
    "            class_numerators[i] = lk_message * self.prior\n",
    "\n",
    "        normalize_term = class_numerators.sum(axis=1).reshape(-1, 1)\n",
    "        conditional_probas = class_numerators / normalize_term\n",
    "        return conditional_probas\n",
    "\n",
    "    def predict(self, X: matrix_like):\n",
    "        \"\"\" Predict class with highest probability \"\"\"\n",
    "        return self.predict_proba(X).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = [GaussianNB(), BernoulliNB(), CategoricalNB(), ComplementNB(), MultinomialNB()]\n",
    "classifiers_names = [\"Gaussian\", \"Bernoulli\", \"Categorical\", \"complement\", \"Multinomial\"]\n",
    "\n",
    "arr = np.zeros((5, 11))\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        tr, te = next(stratified_data)\n",
    "\n",
    "        X_train, y_train, X_test, y_test = train_clf(tr,te)\n",
    "\n",
    "        for i,clf in enumerate(classifiers):\n",
    "            clf.fit(X_train, y_train)\n",
    "            print(clf.score(X_test, y_test))\n",
    "        count += 1\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}