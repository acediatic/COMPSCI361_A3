{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd0e4329b641bbdbd8e1182fbadc5ebb2e523e312e1f9373a057c466e1263c44057",
   "display_name": "Python 3.9.5 64-bit (windows store)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e4329b641bbdbd8e1182fbadc5ebb2e523e312e1f9373a057c466e1263c44057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "p()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"naivebayes-21\\\\trg.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = []\n",
    "class_freq = defaultdict(lambda: 0)\n",
    "corpus = []\n",
    "\n",
    "with open(PATH_TO_DATA) as csv_file:\n",
    "    for line in csv_file:\n",
    "        line = line.replace('\\\"', '')\n",
    "\n",
    "        line_lst = line.split(',')\n",
    "        line_lst[-1] = line_lst[-1].replace('\\n', '')\n",
    "\n",
    "        class_freq[line_lst[1]] += 1 \n",
    "\n",
    "        corpus.append(line_lst[-1])\n",
    "\n",
    "        list_data.append(line_lst)\n",
    "\n",
    "\n",
    "full_csv_data = np.array(list_data)\n",
    "class_freq.pop('class')\n",
    "\n",
    "\n",
    "labels = class_freq.keys()\n",
    "headers = full_csv_data[0]\n",
    "data = np.asarray(full_csv_data[1:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_items([('B', 1602), ('A', 128), ('E', 2144), ('V', 126)])\ndict_keys(['B', 'A', 'E', 'V'])\n"
     ]
    }
   ],
   "source": [
    "print(class_freq.items())\n",
    "print(labels)\n",
    "\n",
    "NUM_CLASSES = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "ABSTRT_I = 1\n",
    "LBL_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances_for_class = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_stratified_kfold_splits(data, k=10):\n",
    "    instances_of_class = dict()\n",
    "\n",
    "    # Create a dictionary with key being label, and the value being an array of instances of that label. \n",
    "    for i, label in enumerate(labels):\n",
    "        i_arr = data[:, 0] == label\n",
    "        instances_of_class[label] = data[i_arr]\n",
    "        num_instances_for_class[i] = len(instances_of_class[label])\n",
    "    \n",
    "    stratified_splits = dict()\n",
    "    \n",
    "    # Upsample to divisible by k\n",
    "    for class_label, class_instances in instances_of_class.items():\n",
    "        n = len(class_instances)\n",
    "        upsample_amt = k - (n % k)\n",
    "\n",
    "        random_indices = np.random.choice(class_instances[:, ABSTRT_I], size=upsample_amt, replace=False)\n",
    "        random_indices.resize((random_indices.shape[0], 2), refcheck=False)\n",
    "\n",
    "        random_indices[:, -1] = class_label\n",
    "\n",
    "        random_indices[:,[0, 1]] = random_indices[:,[1, 0]]\n",
    "\n",
    "        upsampled_class_instances = np.concatenate((class_instances, random_indices), 0)\n",
    "        assert len(upsampled_class_instances) % k == 0, \"num examples should be divisible by k\"\n",
    "\n",
    "        stratified_splits[class_label] = np.split(upsampled_class_instances, k)\n",
    "\n",
    "    for i in range(k):\n",
    "        kth_test_lst = [stratified_splits[label][i] for label in labels]\n",
    "        kth_train_lst = [stratified_splits[label][j] for label in labels for j in range(k) if j != i]\n",
    "        \n",
    "        kth_train_data = np.concatenate(kth_train_lst)\n",
    "        kth_test_data = np.concatenate(kth_test_lst)\n",
    "        np.random.shuffle(kth_train_data)\n",
    "        np.random.shuffle(kth_test_data)\n",
    "\n",
    "        yield kth_train_data, kth_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_data = get_stratified_kfold_splits(data)\n",
    "label_ints = {lbl:i for i, lbl in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_all_words(X):\n",
    "    all_words = set(word for i in range(len(data)) for word in X[i].split() if word) \n",
    "\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_i(all_words):\n",
    "    return {word:i for i, word in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(data):\n",
    "    X = data[:, ABSTRT_I]\n",
    "    return get_all_words_i(get_set_all_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(data, all_words_i):    \n",
    "    # ALPHA = 1\n",
    "    freq_mat = np.ones((len(data), len(all_words_i)+1))\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i][ABSTRT_I].split():\n",
    "            try:\n",
    "                freq_mat[i,all_words_i[word]] += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        freq_mat[i, -1] = label_ints[data[i, LBL_I]]\n",
    "\n",
    "    return freq_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(arr, i):\n",
    "    ''' Calcualtes the correlation between one column (class) and the rest for the input matrix. Credit to FBruzzesi (https://stackoverflow.com/users/12411536/fbruzzesi)'''\n",
    "    mean_t = np.mean(arr, axis=0)\n",
    "    std_t = np.std(arr, axis=0)\n",
    "\n",
    "    mean_i = mean_t[i]\n",
    "    std_i = std_t[i]\n",
    "\n",
    "    mean_xy = np.mean(arr*arr[:,i][:,None], axis=0)\n",
    "\n",
    "    corr = (mean_xy - mean_i * mean_t)/(std_i * std_t)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_best(freq_mat, k=10):\n",
    "    R = corr(freq_mat, -1)\n",
    "    class_correlations = abs(R)\n",
    "\n",
    "    k_best_i = np.argpartition(class_correlations, -(k+1))[-(k+1):]\n",
    "    \n",
    "    # removes self column\n",
    "    k_best_i = k_best_i[:-1]\n",
    "\n",
    "    return k_best_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_clf_form(freq_mat, k_best_i):\n",
    "    X = freq_mat[:, k_best_i]\n",
    "    y = freq_mat[:, -1]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_mat(X, y):\n",
    "    sum_mat = np.zeros(NUM_CLASSES, X.shape[1])\n",
    "\n",
    "    for i in range(NUM_CLASSES):\n",
    "        temp = X[y == i]\n",
    "        sum_mat[i,:] = np.sum(temp, axis=0)\n",
    "\n",
    "    return sum_mat, np.sum(sum_mat, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq_k_best(train, test):\n",
    "    all_words_i = get_all_words(train)\n",
    "\n",
    "    freq_mat_train = get_word_counts(train, all_words_i)\n",
    "    k_best_i = select_k_best(freq_mat_train, NUM_FEATURES)\n",
    "\n",
    "    freq_mat_test = get_word_counts(test, all_words_i)\n",
    "\n",
    "    X_train, y_train = convert_to_clf_form(freq_mat_train, k_best_i)\n",
    "    X_test, y_test = convert_to_clf_form(freq_mat_test, k_best_i)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF\n",
    "def abstract_to_dict(abstract):\n",
    "    '''word:count for words in a particular abstract'''\n",
    "    abstractDict = defaultdict(lambda: 0)\n",
    "    for word in abstract.split(' '):\n",
    "        if word:\n",
    "            abstractDict[word] += 1 \n",
    "    return abstractDict \n",
    "\n",
    "def termFrequency(abstractDict : dict):\n",
    "    \"\"\"(# of repetitions of word in a document) / (# of words in a document)\"\"\"\n",
    "    termFrequencies = {}\n",
    "    numWords = len(abstractDict)\n",
    "    \n",
    "    for word, count in abstractDict.items():\n",
    "        termFrequencies[word] = count/numWords\n",
    "    return termFrequencies\n",
    "\n",
    "def get_inverse_document_frequency(abstract_dict_list : list):\n",
    "    \"\"\" used to calculate the weight of rare words across all documents in the corpus\n",
    "        idf(w) = log(num_docs/freq_word_all_docs)\"\"\"\n",
    "    idf = defaultdict(lambda: 0)\n",
    "    numAbstracts = len(abstract_dict_list)\n",
    "\n",
    "    # calculte number of docs containing word\n",
    "    for abstractDict in abstract_dict_list:\n",
    "        for word, count in abstractDict.items():\n",
    "            if count > 0:\n",
    "                idf[word] += 1 \n",
    "\n",
    "    from math import log10\n",
    "    for word, num_docs_containing_word in idf.items():\n",
    "        idf[word] = log10(numAbstracts/float(num_docs_containing_word))\n",
    "\n",
    "    return idf\n",
    "\n",
    "def get_row_tfidf(tf, idf, word_indexes):\n",
    "    ''' num occurrences of word i in doc j * log(total docs / number of documents containing i) '''\n",
    "    row = np.zeros((1, len(word_indexes)))\n",
    "    for word, numOccurences in tf.items():\n",
    "        word_index = word_indexes[word]\n",
    "        row[0, word_index] = numOccurences*idf[word] \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(X):\n",
    "    abstract_dict_list = []\n",
    "    term_frequency_lst = []\n",
    "    all_words = get_set_all_words(X)\n",
    "\n",
    "    new_X = np.zeros((len(X), len(all_words)))\n",
    "    word_indexes = {word: i for i, word in enumerate(all_words)}\n",
    "\n",
    "    for abstract in X:\n",
    "        abstract_dict = abstract_to_dict(abstract)\n",
    "        abstract_dict_list.append(abstract_dict)\n",
    "        term_frequency_lst.append(termFrequency(abstract_dict))\n",
    "\n",
    "    print(\"converted abstracts, calculating idf\")    \n",
    "    idf = get_inverse_document_frequency(abstract_dict_list)\n",
    "\n",
    "    print(\"calculating tfidf\")\n",
    "    for i,tf in enumerate(term_frequency_lst):\n",
    "        if (i % 500 == 0):\n",
    "            print(\"{}% complete\".format(i/len(abstract_dict_list) * 100))\n",
    "        new_X[i, :] = get_row_tfidf(tf, idf, word_indexes)\n",
    "    print(\"100% complete\")\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "converted abstracts, calculating idf\n",
      "calculating tfidf\n",
      "0.0% complete\n",
      "12.5% complete\n",
      "25.0% complete\n",
      "37.5% complete\n",
      "50.0% complete\n",
      "62.5% complete\n",
      "75.0% complete\n",
      "87.5% complete\n",
      "100% complete\n"
     ]
    }
   ],
   "source": [
    "X = calculate_tfidf(data[:, ABSTRT_I])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(X[X > 0].any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 7048 24988  7534   426 17796 18526 27190 18407 29270 25025  1918 14526\n 21735 12907 10027 11883  5976 19889 29256 19536]\n"
     ]
    }
   ],
   "source": [
    "sum_idf = X.sum(axis=0)\n",
    "k = 20\n",
    "k_best_i = np.argpartition(sum_idf, -k)[-k:]\n",
    "print(k_best_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.01322218 0.00603359 0.         ... 0.01466377 0.         0.        ]\n [0.00415392 0.         0.         ... 0.00767802 0.00678854 0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n ...\n [0.         0.         0.01083682 ... 0.         0.         0.        ]\n [0.00435975 0.00497364 0.00893305 ... 0.         0.         0.00562862]\n [0.         0.00235929 0.         ... 0.         0.00337977 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X = X[:, k_best_i]\n",
    "print(X)\n",
    "y = data[:, LBL_I]"
   ]
  },
  {
   "source": [
    "\"https://towardsdatascience.com/comparing-a-variety-of-naive-bayes-classification-algorithms-fc5fa298379e\"  \n",
    "https://geoffruddock.com/naive-bayes-from-scratch-with-numpy/\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X_train: np.array, y_train: np.array):\n",
    "        self.num_classes = len(np.unique(y_train))\n",
    "        self.num_instances, self.num_features = X_train.shape\n",
    "        self.classes = {i:c for i,c in enumerate(np.unique(y))}\n",
    "\n",
    "        X_by_class = [X[y == c] for c in self.classes.values()]\n",
    "\n",
    "        self.log_cond_by_class = np.zeros((self.num_classes, self.num_features))\n",
    "\n",
    "        self.total_word_count_by_class = np.zeros(((self.num_classes), 1))\n",
    "\n",
    "        for i in range(self.num_classes):\n",
    "            word_freq_fr_class = np.sum(X_by_class[i], axis=0) + self.alpha\n",
    "\n",
    "            assert 0 not in word_freq_fr_class, 'word_freq_should all be > 0'\n",
    "\n",
    "            self.total_word_count_by_class[i] = np.sum(word_freq_fr_class) \n",
    "\n",
    "            assert 0 not in self.total_word_count_by_class[i], 'total_word_count must all be > 0'\n",
    "\n",
    "            self.log_cond_by_class[i, :] = np.log(word_freq_fr_class / self.total_word_count_by_class[i])\n",
    "\n",
    "        total_word_count = np.sum(self.total_word_count_by_class)\n",
    "\n",
    "        self.prior_by_class = np.log(self.total_word_count_by_class / total_word_count)\n",
    "\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y = np.zeros((len(X_test), 1))\n",
    "        for instance_i, instance in enumerate(X_test):\n",
    "            p_by_class = np.copy(self.prior_by_class)\n",
    "\n",
    "            for c in range(self.num_classes):\n",
    "                for word_i in range(self.num_features):\n",
    "                    log_cond_prob = self.log_cond_by_class[c][word_i]\n",
    "\n",
    "                    p_by_class[c] += log_cond_prob * instance[word_i]\n",
    "        \n",
    "            y[instance_i] = np.argmax(p_by_class, axis = 0)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[2.]\n [2.]\n [2.]\n ...\n [2.]\n [2.]\n [2.]]\nFalse\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayes(alpha = 1)\n",
    "clf.fit(X, y)\n",
    "\n",
    "predict_y = clf.predict(X)\n",
    "print(predict_y)\n",
    "print(3 in predict_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}