{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd0e4329b641bbdbd8e1182fbadc5ebb2e523e312e1f9373a057c466e1263c44057",
   "display_name": "Python 3.9.5 64-bit (windows store)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e4329b641bbdbd8e1182fbadc5ebb2e523e312e1f9373a057c466e1263c44057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "p()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"naivebayes-21\\\\trg.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = []\n",
    "class_freq = defaultdict(lambda: 0)\n",
    "corpus = []\n",
    "\n",
    "with open(PATH_TO_DATA) as csv_file:\n",
    "    for line in csv_file:\n",
    "        line = line.replace('\\\"', '')\n",
    "\n",
    "        line_lst = line.split(',')\n",
    "        line_lst[-1] = line_lst[-1].replace('\\n', '')\n",
    "\n",
    "        class_freq[line_lst[1]] += 1 \n",
    "\n",
    "        corpus.append(line_lst[-1])\n",
    "\n",
    "        list_data.append(line_lst)\n",
    "\n",
    "\n",
    "full_csv_data = np.array(list_data)\n",
    "class_freq.pop('class')\n",
    "\n",
    "\n",
    "labels = class_freq.keys()\n",
    "headers = full_csv_data[0]\n",
    "data = np.asarray(full_csv_data[1:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_items([('B', 1602), ('A', 128), ('E', 2144), ('V', 126)])\ndict_keys(['B', 'A', 'E', 'V'])\n"
     ]
    }
   ],
   "source": [
    "print(class_freq.items())\n",
    "print(labels)\n",
    "\n",
    "NUM_CLASSES = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "ABSTRT_I = 1\n",
    "LBL_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances_for_class = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_stratified_kfold_splits(data, k=10):\n",
    "    instances_of_class = dict()\n",
    "\n",
    "    # Create a dictionary with key being label, and the value being an array of instances of that label. \n",
    "    for i, label in enumerate(labels):\n",
    "        i_arr = data[:, 0] == label\n",
    "        instances_of_class[label] = data[i_arr]\n",
    "        num_instances_for_class[i] = len(instances_of_class[label])\n",
    "    \n",
    "    stratified_splits = dict()\n",
    "    \n",
    "    # Upsample to divisible by k\n",
    "    for class_label, class_instances in instances_of_class.items():\n",
    "        n = len(class_instances)\n",
    "        upsample_amt = k - (n % k)\n",
    "\n",
    "        random_indices = np.random.choice(class_instances[:, ABSTRT_I], size=upsample_amt, replace=False)\n",
    "        random_indices.resize((random_indices.shape[0], 2), refcheck=False)\n",
    "\n",
    "        random_indices[:, -1] = class_label\n",
    "\n",
    "        random_indices[:,[0, 1]] = random_indices[:,[1, 0]]\n",
    "\n",
    "        upsampled_class_instances = np.concatenate((class_instances, random_indices), 0)\n",
    "        assert len(upsampled_class_instances) % k == 0, \"num examples should be divisible by k\"\n",
    "\n",
    "        stratified_splits[class_label] = np.split(upsampled_class_instances, k)\n",
    "\n",
    "    for i in range(k):\n",
    "        kth_test_lst = [stratified_splits[label][i] for label in labels]\n",
    "        kth_train_lst = [stratified_splits[label][j] for label in labels for j in range(k) if j != i]\n",
    "        \n",
    "        kth_train_data = np.concatenate(kth_train_lst)\n",
    "        kth_test_data = np.concatenate(kth_test_lst)\n",
    "        np.random.shuffle(kth_train_data)\n",
    "        np.random.shuffle(kth_test_data)\n",
    "\n",
    "        yield kth_train_data, kth_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 'B', 1: 'A', 2: 'E', 3: 'V'}\n"
     ]
    }
   ],
   "source": [
    "stratified_data = get_stratified_kfold_splits(data)\n",
    "label_ints = {lbl:i for i, lbl in enumerate(labels)}\n",
    "class_from_int = {i:lbl for lbl, i in label_ints.items()}\n",
    "print(class_from_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_all_words(X):\n",
    "    all_words = set(word for i in range(len(X)) for word in X[i].split() if word) \n",
    "\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_i(all_words):\n",
    "    return {word:i for i, word in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(data):\n",
    "    X = data[:, ABSTRT_I]\n",
    "    return get_all_words_i(get_set_all_words(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(data, all_words_i):    \n",
    "    # ALPHA = 1\n",
    "    freq_mat = np.ones((len(data), len(all_words_i)+1))\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i][ABSTRT_I].split():\n",
    "            try:\n",
    "                freq_mat[i,all_words_i[word]] += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        freq_mat[i, -1] = label_ints[data[i, LBL_I]]\n",
    "\n",
    "    return freq_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(arr, i):\n",
    "    ''' Calcualtes the correlation between one column (class) and the rest for the input matrix. Credit to FBruzzesi (https://stackoverflow.com/users/12411536/fbruzzesi)'''\n",
    "    mean_t = np.mean(arr, axis=0)\n",
    "    std_t = np.std(arr, axis=0)\n",
    "\n",
    "    mean_i = mean_t[i]\n",
    "    std_i = std_t[i]\n",
    "\n",
    "    mean_xy = np.mean(arr*arr[:,i][:,None], axis=0)\n",
    "\n",
    "    corr = (mean_xy - mean_i * mean_t)/(std_i * std_t)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_best(freq_mat, k=10):\n",
    "    R = corr(freq_mat, -1)\n",
    "    class_correlations = abs(R)\n",
    "\n",
    "    k_best_i = np.argpartition(class_correlations, -(k+1))[-(k+1):]\n",
    "    \n",
    "    # removes self column\n",
    "    k_best_i = k_best_i[:-1]\n",
    "\n",
    "    return k_best_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_clf_form(freq_mat, k_best_i):\n",
    "    X = freq_mat[:, k_best_i]\n",
    "    y = freq_mat[:, -1]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_mat(X, y):\n",
    "    sum_mat = np.zeros(NUM_CLASSES, X.shape[1])\n",
    "\n",
    "    for i in range(NUM_CLASSES):\n",
    "        temp = X[y == i]\n",
    "        sum_mat[i,:] = np.sum(temp, axis=0)\n",
    "\n",
    "    return sum_mat, np.sum(sum_mat, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq_k_best(train, test, k = 30):\n",
    "    all_words_i = get_all_words(train)\n",
    "\n",
    "    freq_mat_train = get_word_counts(train, all_words_i)\n",
    "    k_best_i = select_k_best(freq_mat_train, k)\n",
    "\n",
    "    freq_mat_test = get_word_counts(test, all_words_i)\n",
    "\n",
    "    X_train, y_train = convert_to_clf_form(freq_mat_train, k_best_i)\n",
    "    X_test, y_test = convert_to_clf_form(freq_mat_test, k_best_i)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF\n",
    "def abstract_to_dict(abstract):\n",
    "    '''word:count for words in a particular abstract'''\n",
    "    abstractDict = defaultdict(lambda: 0)\n",
    "    for word in abstract.split(' '):\n",
    "        if word:\n",
    "            abstractDict[word] += 1 \n",
    "    return abstractDict \n",
    "\n",
    "def termFrequency(abstractDict : dict):\n",
    "    \"\"\"(# of repetitions of word in a document) / (# of words in a document)\"\"\"\n",
    "    termFrequencies = {}\n",
    "    numWords = len(abstractDict)\n",
    "    \n",
    "    for word, count in abstractDict.items():\n",
    "        termFrequencies[word] = count/numWords\n",
    "    return termFrequencies\n",
    "\n",
    "def get_inverse_document_frequency(abstract_dict_list : list):\n",
    "    \"\"\" used to calculate the weight of rare words across all documents in the corpus\n",
    "        idf(w) = log(num_docs/freq_word_all_docs)\"\"\"\n",
    "    idf = defaultdict(lambda: 0)\n",
    "    numAbstracts = len(abstract_dict_list)\n",
    "\n",
    "    # calculte number of docs containing word\n",
    "    for abstractDict in abstract_dict_list:\n",
    "        for word, count in abstractDict.items():\n",
    "            if count > 0:\n",
    "                idf[word] += 1 \n",
    "\n",
    "    from math import log10\n",
    "    for word, num_docs_containing_word in idf.items():\n",
    "        idf[word] = np.log(numAbstracts+1/num_docs_containing_word)+1\n",
    "\n",
    "    return idf\n",
    "\n",
    "def get_row_tfidf(tf, idf, word_indexes):\n",
    "    ''' num occurrences of word i in doc j * log(total docs / number of documents containing i) '''\n",
    "    row = np.zeros((1, len(word_indexes)))\n",
    "    for word, numOccurences in tf.items():\n",
    "        word_index = word_indexes[word]\n",
    "        row[0, word_index] = numOccurences*idf[word] \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(X):\n",
    "    abstract_dict_list = []\n",
    "    term_frequency_lst = []\n",
    "    all_words = get_set_all_words(X)\n",
    "\n",
    "    new_X = np.zeros((len(X), len(all_words)))\n",
    "    word_indexes = {word: i for i, word in enumerate(all_words)}\n",
    "\n",
    "    for abstract in X:\n",
    "        abstract_dict = abstract_to_dict(abstract)\n",
    "        abstract_dict_list.append(abstract_dict)\n",
    "        term_frequency_lst.append(termFrequency(abstract_dict))\n",
    "\n",
    "    print(\"converted abstracts, calculating idf\")    \n",
    "    idf = get_inverse_document_frequency(abstract_dict_list)\n",
    "\n",
    "    print(\"calculating tfidf\")\n",
    "    for i,tf in enumerate(term_frequency_lst):\n",
    "        if (i % 500 == 0):\n",
    "            print(\"{}% complete\".format(i/len(abstract_dict_list) * 100))\n",
    "        new_X[i, :] = get_row_tfidf(tf, idf, word_indexes)\n",
    "    print(\"100% complete\")\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = calculate_tfidf(data[:, ABSTRT_I])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_tfidf = X.sum(axis=0)\n",
    "# k = 20\n",
    "# k_best_i = np.argpartition(sum_tfidf, -k)[-k:]\n",
    "\n",
    "# X = X[:, k_best_i]\n",
    "\n",
    "# y = data[:, LBL_I]\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X_train: np.array, y_train: np.array):\n",
    "        self.num_classes = len(np.unique(y_train))\n",
    "        self.num_instances, self.num_features = X_train.shape\n",
    "\n",
    "        # splits X into a list of arrays containing instances of a particular class\n",
    "        instances_from_class = [X_train[y_train == c] for c in range(self.num_classes)]\n",
    "\n",
    "        # initalises log cond probability array\n",
    "        self.log_cond_by_class = np.zeros((self.num_classes, self.num_features))\n",
    "\n",
    "        # initalises total_word_count_by_class array\n",
    "        self.total_word_count_by_class = np.zeros((self.num_classes, 1))\n",
    "\n",
    "        # initialises num examples by class\n",
    "        self.num_examples_in_class = np.zeros((self.num_classes, 1))\n",
    "\n",
    "        for c in range(self.num_classes):\n",
    "            word_freq_for_class = np.sum(instances_from_class[c], axis=0) + self.alpha\n",
    "            assert 0 not in word_freq_for_class, 'word_freq_should all be > 0'\n",
    "\n",
    "            self.total_word_count_by_class[c] = np.sum(word_freq_for_class) \n",
    "            assert 0 not in self.total_word_count_by_class[c], 'total_word_count must all be > 0'\n",
    "\n",
    "            self.log_cond_by_class[c, :] = np.log(word_freq_for_class / self.total_word_count_by_class[c])\n",
    "\n",
    "            self.num_examples_in_class[c] = instances_from_class[c].shape[0]\n",
    "\n",
    "        total_word_count = np.sum(self.total_word_count_by_class)\n",
    "\n",
    "        self.prior_by_class = np.log(self.num_examples_in_class / self.num_instances)\n",
    "\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        num_instances = len(X_test)\n",
    "        y = np.zeros(num_instances)\n",
    "\n",
    "        for i in range(num_instances):\n",
    "            p_by_class = np.copy(self.prior_by_class)\n",
    "\n",
    "            for c in range(self.num_classes):\n",
    "                for word_i in range(self.num_features):\n",
    "                    log_cond_prob = self.log_cond_by_class[c][word_i]\n",
    "\n",
    "                    freq = X_test[i,word_i]\n",
    "                    p_by_class[c] += log_cond_prob * freq\n",
    "        \n",
    "            y[i] = np.argmax(p_by_class, axis = 0)[0]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GaussianNaiveBayes():\n",
    "\n",
    "#     def __init__(self, alpha=1):\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#     def gaussian_pdf(self, class_num, x):\n",
    "#         mean = self.mean[class_num]\n",
    "#         var = self.var[class_num]\n",
    "#         numerator = np.exp(-((x-mean)**2) / (2 * var))\n",
    "#         denominator = np.sqrt(2 * np.pi * var)\n",
    "#         prob = numerator / denominator\n",
    "#         return prob\n",
    "\n",
    "#     def fit(self, X_train: np.array, y_train: np.array):\n",
    "#         self.num_classes = len(np.unique(y_train))\n",
    "#         self.num_instances, self.num_features = X_train.shape\n",
    "\n",
    "#         # splits X into a list of arrays containing instances of a particular class\n",
    "#         instances_from_class = [X_train[y_train == c] for c in self.classes.values()]\n",
    "\n",
    "#         # initalises log cond probability array\n",
    "#         self.log_cond_by_class = np.zeros((self.num_classes, self.num_features))\n",
    "\n",
    "#         # initalises total_word_count_by_class array\n",
    "#         self.total_word_count_by_class = np.zeros((self.num_classes, 1))\n",
    "\n",
    "#         # initialises num examples by class\n",
    "#         self.num_examples_in_class = np.zeros((self.num_classes, 1))\n",
    "\n",
    "#         for c in range(self.num_classes):\n",
    "#             word_freq_for_class = np.sum(instances_from_class[c], axis=0) + self.alpha\n",
    "#             assert 0 not in word_freq_for_class, 'word_freq_should all be > 0'\n",
    "\n",
    "#             self.total_word_count_by_class[c] = np.sum(word_freq_for_class) \n",
    "#             assert 0 not in self.total_word_count_by_class[c], 'total_word_count must all be > 0'\n",
    "\n",
    "#             self.num_examples_in_class[c] = instances_from_class[c].shape[0]\n",
    "\n",
    "#         total_word_count = np.sum(self.total_word_count_by_class)\n",
    "\n",
    "#         self.mean = np.array(np.mean(instances_from_class[c] for c in range(self.num_classes))\n",
    "\n",
    "#         self.var = np.array(np.var(instances_from_class[c] for c in range(self.num_classes))\n",
    "\n",
    "\n",
    "#         self.prior_by_class = np.log(self.num_examples_in_class / self.num_instances)\n",
    "\n",
    "#         for c in range(self.num_classes):\n",
    "#             self.log_cond_by_class[c, :] = gaussian_pdf(x)\n",
    "\n",
    "    \n",
    "#     def predict(self, X_test):\n",
    "#         num_instances = len(X_test)\n",
    "#         y = np.zeros((num_instances), dtype=str)\n",
    "\n",
    "#         for i in range(num_instances):\n",
    "#             p_by_class = np.copy(self.prior_by_class)\n",
    "\n",
    "#             for c in range(self.num_classes):\n",
    "#                 for word_i in range(self.num_features):\n",
    "#                     log_cond_prob = self.log_cond_by_class[c][word_i]\n",
    "\n",
    "#                     freq = X_test[i,word_i]\n",
    "#                     p_by_class[c] += log_cond_prob * freq\n",
    "        \n",
    "#             y[i] = self.classes[]\n",
    "#         return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_test_classifier(X_train, y_train, X_test, y_test):\n",
    "    clf = NaiveBayes(alpha = 1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predict_y = clf.predict(X_test)\n",
    "\n",
    "    accuracy = y_test[predict_y == y_test].sum()/len(y_test)\n",
    "\n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv():\n",
    "    results = 0\n",
    "    count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            train, test = next(stratified_data)\n",
    "            X_train, y_train, X_test, y_test = word_freq_k_best(train, test, k=30)\n",
    "            accuracy = fit_and_test_classifier(X_train, y_train, X_test, y_test)\n",
    "            results += accuracy\n",
    "            count += 1\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    print(\"Classifier Accuracy: \", results/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[57670.]\n",
      " [ 4366.]\n",
      " [66537.]\n",
      " [ 4173.]]\n",
      "0.9502487562189055\n",
      "[[57960.]\n",
      " [ 4403.]\n",
      " [67318.]\n",
      " [ 4185.]]\n",
      "0.9502487562189055\n",
      "[[57837.]\n",
      " [ 4374.]\n",
      " [67238.]\n",
      " [ 4217.]]\n",
      "0.9477611940298507\n",
      "[[57746.]\n",
      " [ 4363.]\n",
      " [67108.]\n",
      " [ 4173.]]\n",
      "0.9875621890547264\n",
      "[[57793.]\n",
      " [ 4391.]\n",
      " [67262.]\n",
      " [ 4178.]]\n",
      "0.8880597014925373\n",
      "[[57456.]\n",
      " [ 4381.]\n",
      " [67156.]\n",
      " [ 4263.]]\n",
      "0.9850746268656716\n",
      "[[57634.]\n",
      " [ 4391.]\n",
      " [66544.]\n",
      " [ 4246.]]\n",
      "0.927860696517413\n",
      "[[57653.]\n",
      " [ 4372.]\n",
      " [67257.]\n",
      " [ 4254.]]\n",
      "0.9228855721393034\n",
      "[[57666.]\n",
      " [ 4383.]\n",
      " [66443.]\n",
      " [ 4185.]]\n",
      "0.9353233830845771\n",
      "[[57860.]\n",
      " [ 4372.]\n",
      " [66418.]\n",
      " [ 4211.]]\n",
      "0.9104477611940298\n",
      "0.940547263681592\n"
     ]
    }
   ],
   "source": [
    "cv()"
   ]
  }
 ]
}