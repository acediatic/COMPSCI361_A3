{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd0e4329b641bbdbd8e1182fbadc5ebb2e523e312e1f9373a057c466e1263c44057",
   "display_name": "Python 3.9.5 64-bit (windows store)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e4329b641bbdbd8e1182fbadc5ebb2e523e312e1f9373a057c466e1263c44057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "p()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"naivebayes-21\\\\trg.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = []\n",
    "class_freq = defaultdict(lambda: 0)\n",
    "corpus = []\n",
    "\n",
    "with open(PATH_TO_DATA) as csv_file:\n",
    "    for line in csv_file:\n",
    "        line = line.replace('\\\"', '')\n",
    "\n",
    "        line_lst = line.split(',')\n",
    "        line_lst[-1] = line_lst[-1].replace('\\n', '')\n",
    "\n",
    "        class_freq[line_lst[1]] += 1 \n",
    "\n",
    "        corpus.append(line_lst[-1])\n",
    "\n",
    "        list_data.append(line_lst)\n",
    "\n",
    "\n",
    "full_csv_data = np.array(list_data)\n",
    "class_freq.pop('class')\n",
    "\n",
    "\n",
    "labels = class_freq.keys()\n",
    "headers = full_csv_data[0]\n",
    "data = np.asarray(full_csv_data[1:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_items([('B', 1602), ('A', 128), ('E', 2144), ('V', 126)])\ndict_keys(['B', 'A', 'E', 'V'])\n"
     ]
    }
   ],
   "source": [
    "print(class_freq.items())\n",
    "print(labels)\n",
    "\n",
    "NUM_CLASSES = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "ABSTRT_I = 1\n",
    "LBL_I = 0\n",
    "\n",
    "class_to_int = {lbl:i for i, lbl in enumerate(labels)}\n",
    "int_to_class = {i:lbl for lbl, i in class_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_stratified_kfold_splits(data, k=10):\n",
    "    instances_of_class = dict()\n",
    "\n",
    "    # Create a dictionary with key being label, and the value being an array of instances of that class. \n",
    "    for i, label in enumerate(labels):\n",
    "        instances_of_class[label] = data[data[:, 0] == label]\n",
    "    \n",
    "    stratified_splits = dict()\n",
    "    \n",
    "    # Upsample to divisible by k\n",
    "    for class_label, class_instances in instances_of_class.items():\n",
    "        n = len(class_instances)\n",
    "        upsample_amt = k - (n % k)\n",
    "\n",
    "        random_indices = np.random.choice(class_instances[:, ABSTRT_I], size=upsample_amt, replace=False)\n",
    "        random_indices.resize((random_indices.shape[0], 2), refcheck=False)\n",
    "\n",
    "        random_indices[:, -1] = class_label\n",
    "\n",
    "        random_indices[:,[0, 1]] = random_indices[:,[1, 0]]\n",
    "\n",
    "        upsampled_class_instances = np.concatenate((class_instances, random_indices), 0)\n",
    "        assert len(upsampled_class_instances) % k == 0, \"num examples should be divisible by k\"\n",
    "\n",
    "        stratified_splits[class_label] = np.split(upsampled_class_instances, k)\n",
    "\n",
    "    for i in range(k):\n",
    "        kth_test_lst = [stratified_splits[label][i] for label in labels]\n",
    "        kth_train_lst = [stratified_splits[label][j] for label in labels for j in range(k) if j != i]\n",
    "\n",
    "        # Check golden rule preserved\n",
    "        for label in labels:\n",
    "            try:\n",
    "                kth_train_lst.index(stratified_splits[label][i])\n",
    "                assert False, \"GOLDEN RULE BROKEN!\" \n",
    "            except ValueError:\n",
    "                pass  \n",
    "    \n",
    "        kth_train_data = np.concatenate(kth_train_lst)\n",
    "        kth_test_data = np.concatenate(kth_test_lst)\n",
    "        np.random.shuffle(kth_train_data)\n",
    "        np.random.shuffle(kth_test_data)\n",
    "\n",
    "        yield kth_train_data, kth_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 'B', 1: 'A', 2: 'E', 3: 'V'}\n"
     ]
    }
   ],
   "source": [
    "stratified_data = get_stratified_kfold_splits(data)\n",
    "\n",
    "print(int_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq_k_best(train, test, k = 500):\n",
    "    X_train, y_train = get_X_y(train)\n",
    "    X_test, y_test = get_X_y(test)\n",
    "\n",
    "    train_words_i = get_word_indexes(X_train)\n",
    "\n",
    "    frequency_matrix_train = get_word_counts(X_train, train_words_i)\n",
    "    k_best_i = select_k_best(frequency_matrix_train, corr = False, k=k)\n",
    "\n",
    "    frequency_matrix_test = get_word_counts(X_test, train_words_i)\n",
    "\n",
    "    X_train = get_k_best(frequency_matrix_train, k_best_i)\n",
    "    X_test = get_k_best(frequency_matrix_test, k_best_i)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_indexes(all_words : set):\n",
    "    all_words_set = get_set_all_words(all_words)\n",
    "    return {word:i for i, word in enumerate(all_words_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_all_words(X):\n",
    "    all_words = set(word for i in range(len(X)) for word in X[i].split() if word) \n",
    "\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(abstracts, word_indexes):    \n",
    "\n",
    "    word_frequencies_matrix = np.ones((len(abstracts), len(word_indexes)))\n",
    "\n",
    "    for i in range(len(abstracts)):\n",
    "        for word in abstracts[i].split():\n",
    "            try:\n",
    "                word_frequencies_matrix[i, word_indexes[word]] += 1\n",
    "            except KeyError:\n",
    "                # word not in training words\n",
    "                pass\n",
    "\n",
    "    return word_frequencies_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(arr, i):\n",
    "    ''' Calcualtes the correlation between one column (class) and the rest for the input matrix. Credit to FBruzzesi (https://stackoverflow.com/users/12411536/fbruzzesi)'''\n",
    "    mean_t = np.mean(arr, axis=0)\n",
    "    std_t = np.std(arr, axis=0)\n",
    "\n",
    "    mean_i = mean_t[i]\n",
    "    std_i = std_t[i]\n",
    "\n",
    "    mean_xy = np.mean(arr*arr[:,i][:,None], axis=0)\n",
    "\n",
    "    corr = (mean_xy - mean_i * mean_t)/(std_i * std_t)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_best(word_frequencies_matrix, corr=False, k=10):\n",
    "    if corr:\n",
    "        R = corr(freq_mat, -1)\n",
    "        class_correlations = abs(R)\n",
    "        k_best_i = np.argpartition(class_correlations, -(k+1))[-(k+1):]\n",
    "        \n",
    "        # removes self column\n",
    "        k_best_i = k_best_i[:-1]\n",
    "\n",
    "    else:\n",
    "        class_correlations = word_frequencies_matrix.sum(axis=0)  \n",
    "        k_best_i = np.argpartition(class_correlations, -k)[-k:]\n",
    "    \n",
    "    return k_best_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_best(freq_mat, k_best_i):\n",
    "    X = freq_mat[:, k_best_i]\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(combinedXy):\n",
    "    return combinedXy[:, ABSTRT_I], combinedXy[:, LBL_I]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF\n",
    "def abstract_to_dict(abstract):\n",
    "    '''word:count for words in a particular abstract'''\n",
    "    abstractDict = defaultdict(lambda: 0)\n",
    "    for word in abstract.split(' '):\n",
    "        if word:\n",
    "            abstractDict[word] += 1 \n",
    "    return abstractDict \n",
    "\n",
    "def termFrequency(abstractDict : dict):\n",
    "    \"\"\"(# of repetitions of word in a document) / (# of words in a document)\"\"\"\n",
    "    termFrequencies = {}\n",
    "    numWords = len(abstractDict)\n",
    "    \n",
    "    for word, count in abstractDict.items():\n",
    "        termFrequencies[word] = count/numWords\n",
    "    return termFrequencies\n",
    "\n",
    "def get_inverse_document_frequency(abstract_dict_list : list):\n",
    "    \"\"\" used to calculate the weight of rare words across all documents in the corpus\n",
    "        idf(w) = log(num_docs/freq_word_all_docs)\"\"\"\n",
    "    idf = defaultdict(lambda: 0)\n",
    "    numAbstracts = len(abstract_dict_list)\n",
    "\n",
    "    # calculte number of docs containing word\n",
    "    for abstractDict in abstract_dict_list:\n",
    "        for word, count in abstractDict.items():\n",
    "            if count > 0:\n",
    "                idf[word] += 1 \n",
    "\n",
    "    from math import log10\n",
    "    for word, num_docs_containing_word in idf.items():\n",
    "        idf[word] = np.log(numAbstracts+1/num_docs_containing_word)+1\n",
    "\n",
    "    return idf\n",
    "\n",
    "def get_row_tfidf(tf, idf, word_indexes):\n",
    "    ''' num occurrences of word i in doc j * log(total docs / number of documents containing i) '''\n",
    "    row = np.zeros((1, len(word_indexes)))\n",
    "    for word, numOccurences in tf.items():\n",
    "        word_index = word_indexes[word]\n",
    "        row[0, word_index] = numOccurences*idf[word] \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(X):\n",
    "    abstract_dict_list = []\n",
    "    term_frequency_lst = []\n",
    "    all_words = get_set_all_words(X)\n",
    "\n",
    "    new_X = np.zeros((len(X), len(all_words)))\n",
    "    word_indexes = {word: i for i, word in enumerate(all_words)}\n",
    "\n",
    "    for abstract in X:\n",
    "        abstract_dict = abstract_to_dict(abstract)\n",
    "        abstract_dict_list.append(abstract_dict)\n",
    "        term_frequency_lst.append(termFrequency(abstract_dict))\n",
    "\n",
    "    print(\"converted abstracts, calculating idf\")    \n",
    "    idf = get_inverse_document_frequency(abstract_dict_list)\n",
    "\n",
    "    print(\"calculating tfidf\")\n",
    "    for i,tf in enumerate(term_frequency_lst):\n",
    "        new_X[i] = get_row_tfidf(tf, idf, word_indexes)\n",
    "\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X_train: np.array, y_train: np.array):\n",
    "        self.num_classes = len(np.unique(y_train))\n",
    "        self.num_instances, self.num_features = X_train.shape\n",
    "\n",
    "        self.classes_to_int = {label:i for i, label in enumerate(np.unique(y_train))}\n",
    "        self.int_to_classes = {i:label for label, i in self.classes_to_int.items()}\n",
    "\n",
    "        y_train = np.asarray([self.classes_to_int[label] for label in y_train])\n",
    "\n",
    "        # initalises log cond probability array\n",
    "        self.log_cond_by_class = np.zeros((self.num_classes, self.num_features))\n",
    "\n",
    "        # initalises total_word_count_by_class array\n",
    "        self.total_word_count_by_class = np.zeros((self.num_classes, 1))\n",
    "\n",
    "        # initialises num examples by class\n",
    "        self.num_examples_in_class = np.zeros((self.num_classes, 1))\n",
    "\n",
    "        for c in range(self.num_classes):\n",
    "            # splits X into a list of arrays containing instances of a particular class\n",
    "            mask = (y_train == c)\n",
    "            instances_from_class = X_train[mask,:]\n",
    "\n",
    "            word_freq_for_class = np.sum(instances_from_class, axis=0) + self.alpha\n",
    "            assert 0 not in word_freq_for_class, 'word_freq_should all be > 0'\n",
    "\n",
    "            self.total_word_count_by_class[c] = np.sum(word_freq_for_class) \n",
    "            assert 0 not in self.total_word_count_by_class[c], 'total_word_count must all be > 0'\n",
    "\n",
    "            self.log_cond_by_class[c, :] = np.log(word_freq_for_class / self.total_word_count_by_class[c])\n",
    "\n",
    "            self.num_examples_in_class[c] = instances_from_class.shape[0]\n",
    "\n",
    "        total_word_count = np.sum(self.total_word_count_by_class)\n",
    "\n",
    "        self.prior_by_class = np.log(self.num_examples_in_class / self.num_instances)\n",
    "\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        num_instances = len(X_test)\n",
    "        y = np.zeros(num_instances)\n",
    "\n",
    "        for i in range(num_instances):\n",
    "            p_by_class = np.copy(self.prior_by_class)\n",
    "\n",
    "            for c in range(self.num_classes):\n",
    "                for word_i in range(self.num_features):\n",
    "                    log_cond_prob = self.log_cond_by_class[c][word_i]\n",
    "\n",
    "                    freq = X_test[i,word_i]\n",
    "                    p_by_class[c] += log_cond_prob * freq\n",
    "        \n",
    "            y[i] = np.argmax(p_by_class, axis = 0)[0]\n",
    "        return np.asarray([self.int_to_classes[c] for c in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha = 1):\n",
    "        self.alpha = alpha        \n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        self.class_nums = list(np.unique(y_train))\n",
    "        self.n_classes = len(self.class_nums)    \n",
    "        self.n_examples = X_train.shape[0]\n",
    "        self.n_features = X_train.shape[1]\n",
    "                           \n",
    "        self.log_cond_probs = dict()\n",
    "        self.log_prior_probs = dict()\n",
    "\n",
    "        print(\"Y: \", y_train.shape)\n",
    "        print(\"X: \", X_train.shape)\n",
    "\n",
    "        for class_num in self.class_nums:\n",
    "            #  Get all examples of this class\n",
    "            mask = (y_train == class_num)\n",
    "            class_examples = X_train[mask,:]\n",
    "\n",
    "            print(\"CLASS EX SHAPE\", class_examples.shape)\n",
    "\n",
    "            #  Get the frequency of EACH feature across all examples (rows), and add alpha to each\n",
    "            word_frequencies = class_examples.sum(axis=0) + self.alpha\n",
    "\n",
    "            #  Calculate the log of the conditional probabilities\n",
    "            self.log_cond_probs[class_num] = np.log(word_frequencies / word_frequencies.sum())\n",
    "\n",
    "            #  Calculate the total number of examples in this class (for prior calculation). Note this is not influenced by alpha\n",
    "            n_class_examples = mask.sum()\n",
    "            \n",
    "            # Calculate the prior probability for this class\n",
    "            self.log_prior_probs[class_num] = np.log(n_class_examples / self.n_examples)\n",
    "    \n",
    "\n",
    "        def gaussian_pdf(self, class_num, x):\n",
    "            mean = self.mean[class_num]\n",
    "            var = self.var[class_num]\n",
    "            numerator = np.exp(-((x-mean)**2) / (2 * var))\n",
    "            denominator = np.sqrt(2 * np.pi * var)\n",
    "            prob = numerator / denominator\n",
    "            return prob\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        result = []\n",
    "        for example in X_test:\n",
    "            #  Initialise the class probabilities to the prior probabilities\n",
    "            classes = self.log_prior_probs.copy()\n",
    "            \n",
    "            #  For each class...\n",
    "            for class_no in self.class_nums:\n",
    "            \n",
    "                #  For each word in the example...\n",
    "                for i in range(X_test.shape[1]):  \n",
    "                    \n",
    "                    #  Get the log of conditional probability of this word occurance given the current class\n",
    "                    log_cond_prob = self.log_cond_probs[class_no][i]\n",
    "                    \n",
    "                    #  For each occurance of the word...add the log conditional probability to the current log class probability\n",
    "                    \n",
    "                    count = example[i]\n",
    "                    classes[class_no] += count * log_cond_prob\n",
    "            \n",
    "            # Classify the example as the class with the greatest 'score' value\n",
    "            result.append(max(classes.items(), key = lambda x: x[1])[0])\n",
    "        \n",
    "        #  Return the class with the highest 'score' metric.\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GaussianNaiveBayes():\n",
    "\n",
    "#     def __init__(self, alpha=1):\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#     def gaussian_pdf(self, class_num, x):\n",
    "#         mean = self.mean[class_num]\n",
    "#         var = self.var[class_num]\n",
    "#         numerator = np.exp(-((x-mean)**2) / (2 * var))\n",
    "#         denominator = np.sqrt(2 * np.pi * var)\n",
    "#         prob = numerator / denominator\n",
    "#         return prob\n",
    "\n",
    "#     def fit(self, X_train: np.array, y_train: np.array):\n",
    "#         self.num_classes = len(np.unique(y_train))\n",
    "#         self.num_instances, self.num_features = X_train.shape\n",
    "\n",
    "#         # splits X into a list of arrays containing instances of a particular class\n",
    "#         instances_from_class = [X_train[y_train == c] for c in self.classes.values()]\n",
    "\n",
    "#         # initalises log cond probability array\n",
    "#         self.log_cond_by_class = np.zeros((self.num_classes, self.num_features))\n",
    "\n",
    "#         # initalises total_word_count_by_class array\n",
    "#         self.total_word_count_by_class = np.zeros((self.num_classes, 1))\n",
    "\n",
    "#         # initialises num examples by class\n",
    "#         self.num_examples_in_class = np.zeros((self.num_classes, 1))\n",
    "\n",
    "#         for c in range(self.num_classes):\n",
    "#             word_freq_for_class = np.sum(instances_from_class[c], axis=0) + self.alpha\n",
    "#             assert 0 not in word_freq_for_class, 'word_freq_should all be > 0'\n",
    "\n",
    "#             self.total_word_count_by_class[c] = np.sum(word_freq_for_class) \n",
    "#             assert 0 not in self.total_word_count_by_class[c], 'total_word_count must all be > 0'\n",
    "\n",
    "#             self.num_examples_in_class[c] = instances_from_class[c].shape[0]\n",
    "\n",
    "#         total_word_count = np.sum(self.total_word_count_by_class)\n",
    "\n",
    "#         self.mean = np.array(np.mean(instances_from_class[c] for c in range(self.num_classes))\n",
    "\n",
    "#         self.var = np.array(np.var(instances_from_class[c] for c in range(self.num_classes))\n",
    "\n",
    "\n",
    "#         self.prior_by_class = np.log(self.num_examples_in_class / self.num_instances)\n",
    "\n",
    "#         for c in range(self.num_classes):\n",
    "#             self.log_cond_by_class[c, :] = gaussian_pdf(x)\n",
    "\n",
    "    \n",
    "#     def predict(self, X_test):\n",
    "#         num_instances = len(X_test)\n",
    "#         y = np.zeros((num_instances), dtype=str)\n",
    "\n",
    "#         for i in range(num_instances):\n",
    "#             p_by_class = np.copy(self.prior_by_class)\n",
    "\n",
    "#             for c in range(self.num_classes):\n",
    "#                 for word_i in range(self.num_features):\n",
    "#                     log_cond_prob = self.log_cond_by_class[c][word_i]\n",
    "\n",
    "#                     freq = X_test[i,word_i]\n",
    "#                     p_by_class[c] += log_cond_prob * freq\n",
    "        \n",
    "#             y[i] = self.classes[]\n",
    "#         return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_test_classifier(X_train, y_train, X_test, y_test):\n",
    "    clf = NaiveBayes(alpha = 1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predict_y = clf.predict(X_test)\n",
    "\n",
    "    accuracy = np.count_nonzero(y_test[predict_y == y_test])/len(y_test)\n",
    "\n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_get_X_y(train, test, k=None):\n",
    "    X_train = train[:, ABSTRT_I]\n",
    "    y_train = train[:, LBL_I]\n",
    "\n",
    "    print(X_train.shape)\n",
    "\n",
    "    X_test = test[:, ABSTRT_I]\n",
    "    y_test = test[:, LBL_I]\n",
    "\n",
    "    X_train = calculate_tfidf(X_train)\n",
    "    X_test = calculate_tfidf(X_test) \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(k = 500, tfidf = False):\n",
    "    results = 0\n",
    "    count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            train, test = next(stratified_data)\n",
    "            print('-'*10, \"Run {}\".format(count+1), '-'*10)\n",
    "            print(\"Calculating Word Frequencies\")\n",
    "            feature_fn = tfidf_get_X_y if tfidf else word_freq_k_best \n",
    "            X_train, y_train, X_test, y_test = feature_fn(train, test, k=k)\n",
    "\n",
    "            print(\"Fitting and Testing.\")\n",
    "            accuracy = fit_and_test_classifier(X_train, y_train, X_test, y_test)\n",
    "            results += accuracy\n",
    "            print(\"Fold Accuracy: \", accuracy)\n",
    "            count += 1\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    print('-'*10, \"Complete\", \"-\"*10)\n",
    "    print(\"Classifier Accuracy: \", results/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------- Run 1 ----------\n",
      "Calculating Word Frequencies\n",
      "Fitting and Testing.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NaiveBayes' object has no attribute 'int_to_class'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-131957913993>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-ed16965455c6>\u001b[0m in \u001b[0;36mcv\u001b[1;34m(k, tfidf)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting and Testing.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_and_test_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mresults\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fold Accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-6f59209f1e2c>\u001b[0m in \u001b[0;36mfit_and_test_classifier\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpredict_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredict_y\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-94137468ed38>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X_test)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_by_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint_to_class\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-94137468ed38>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_by_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint_to_class\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NaiveBayes' object has no attribute 'int_to_class'"
     ]
    }
   ],
   "source": [
    "cv(1500, tfidf = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}